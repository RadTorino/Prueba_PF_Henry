{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL de la Data Business Starbucks Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def procesar_y_guardar_starbucks(pkl_path, save_directory):\n",
    "    # Cargar el archivo .pkl en un DataFrame de Pandas\n",
    "    df_business_starbucks = pd.read_pickle(pkl_path)\n",
    "\n",
    "    # Eliminar las columnas duplicadas\n",
    "    df_business_starbucks = df_business_starbucks.loc[:, ~df_business_starbucks.columns.duplicated()]\n",
    "\n",
    "    # Filtrar el DataFrame para incluir solo las filas donde 'name' contiene la palabra 'Starbucks'\n",
    "    df_business_starbucks = df_business_starbucks[df_business_starbucks['name'].str.contains('Starbucks', na=False)]\n",
    "\n",
    "    # Crear la nueva columna 'starbucks_id' concatenando las columnas especificadas, separadas por un guion\n",
    "    df_business_starbucks['starbucks_id'] = df_business_starbucks['state'].astype(str) + '-' + \\\n",
    "                                   df_business_starbucks['latitude'].astype(str) + '-' + \\\n",
    "                                   df_business_starbucks['longitude'].astype(str)\n",
    "\n",
    "    # Convertir la columna 'postal_code' a tipo numérico (float)\n",
    "    df_business_starbucks['postal_code'] = pd.to_numeric(df_business_starbucks['postal_code'], errors='coerce')\n",
    "\n",
    "    # Convertir la columna 'city' a tipo str\n",
    "    df_business_starbucks['city'] = df_business_starbucks['city'].astype(str)\n",
    "\n",
    "    # Eliminar las filas donde la columna 'name' tiene valores NaN\n",
    "    df_business_starbucks = df_business_starbucks.dropna(subset=['name'])\n",
    "\n",
    "    # Eliminar las columnas 'attributes' y 'hours'\n",
    "    df_business_starbucks.drop(columns=['attributes', 'hours'], inplace=True)\n",
    "\n",
    "    # Restablecer el índice\n",
    "    df_business_starbucks = df_business_starbucks.reset_index(drop=True)\n",
    "\n",
    "    # Obtener la fecha actual en formato YYYY-MM-DD\n",
    "    fecha_actual = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Crear el nombre del archivo\n",
    "    nombre_archivo = f'business_starbucks_{fecha_actual}.parquet'\n",
    "\n",
    "    # Guardar el DataFrame como archivo Parquet en el directorio especificado\n",
    "    df_business_starbucks.to_parquet(f'{save_directory}/{nombre_archivo}')\n",
    "\n",
    "    def guardar_y_comparar(df):\n",
    "        # Obtener la fecha actual en formato YYYY-MM-DD\n",
    "        fecha_actual = datetime.now().strftime('%Y-%m-%d')\n",
    "        # Crear el nombre del archivo\n",
    "        nombre_archivo = f'business_starbucks_{fecha_actual}.parquet'\n",
    "        # Guardar el DataFrame como archivo Parquet en el directorio especificado\n",
    "        df.to_parquet(f'{save_directory}/{nombre_archivo}')\n",
    "\n",
    "        # Comprobar si el archivo existe\n",
    "        archivo_antiguo = f'{save_directory}/business_starbucks.parquet'\n",
    "        if os.path.exists(archivo_antiguo):\n",
    "            # Cargar el archivo existente\n",
    "            df_antiguo = pd.read_parquet(archivo_antiguo)\n",
    "            \n",
    "            # Concatenar los datos que no están presentes en el archivo existente\n",
    "            df_nuevo = pd.concat([df_antiguo, df]).drop_duplicates().reset_index(drop=True)\n",
    "            \n",
    "            # Guardar el DataFrame actualizado con el mismo nombre\n",
    "            df_nuevo.to_parquet(archivo_antiguo)\n",
    "            \n",
    "            # Verificar si se actualizó la data\n",
    "            if len(df) > 0:\n",
    "                print(\"Se ha actualizado la data.\")\n",
    "            else:\n",
    "                print(\"No hay nuevos datos para actualizar.\")\n",
    "        else:\n",
    "            print(\"No se encontró el archivo antiguo. Guardando el nuevo archivo.\")\n",
    "\n",
    "    # Usar la función con el DataFrame procesado\n",
    "    guardar_y_comparar(df_business_starbucks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se ha actualizado la data.\n"
     ]
    }
   ],
   "source": [
    "# Ejecucion de la Funcion\n",
    "procesar_y_guardar_starbucks('../data/business.pkl', '../gcp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL de la Data Business Dunkin Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def procesar_y_guardar_dunkin(pkl_path, save_directory):\n",
    "    # Cargar el archivo .pkl en un DataFrame de Pandas\n",
    "    df_business_dunkin = pd.read_pickle(pkl_path)\n",
    "\n",
    "    # Eliminar las columnas duplicadas\n",
    "    df_business_dunkin = df_business_dunkin.loc[:, ~df_business_dunkin.columns.duplicated()]\n",
    "\n",
    "    # Filtrar el DataFrame para incluir solo las filas donde 'name' contiene la palabra 'Dunkin'\n",
    "    df_business_dunkin = df_business_dunkin[df_business_dunkin['name'].str.contains('Dunkin', na=False)]\n",
    "\n",
    "    # Crear la nueva columna 'dunkin_id' concatenando las columnas especificadas, separadas por un guion\n",
    "    df_business_dunkin['dunkin_id'] = df_business_dunkin['state'].astype(str) + '-' + \\\n",
    "                                   df_business_dunkin['latitude'].astype(str) + '-' + \\\n",
    "                                   df_business_dunkin['longitude'].astype(str)\n",
    "\n",
    "    # Convertir la columna 'postal_code' a tipo numérico (float)\n",
    "    df_business_dunkin['postal_code'] = pd.to_numeric(df_business_dunkin['postal_code'], errors='coerce')\n",
    "\n",
    "    # Convertir la columna 'city' a tipo str\n",
    "    df_business_dunkin['city'] = df_business_dunkin['city'].astype(str)\n",
    "\n",
    "    # Eliminar las filas donde la columna 'name' tiene valores NaN\n",
    "    df_business_dunkin = df_business_dunkin.dropna(subset=['name'])\n",
    "\n",
    "    # Eliminar las columnas 'attributes' y 'hours'\n",
    "    df_business_dunkin.drop(columns=['attributes', 'hours'], inplace=True)\n",
    "\n",
    "    # Restablecer el índice\n",
    "    df_business_dunkin = df_business_dunkin.reset_index(drop=True)\n",
    "\n",
    "    # Obtener la fecha actual en formato YYYY-MM-DD\n",
    "    fecha_actual = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Crear el nombre del archivo\n",
    "    nombre_archivo = f'business_dunkin_{fecha_actual}.parquet'\n",
    "\n",
    "    # Guardar el DataFrame como archivo Parquet en el directorio especificado\n",
    "    df_business_dunkin.to_parquet(f'{save_directory}/{nombre_archivo}')\n",
    "\n",
    "    def guardar_y_comparar(df):\n",
    "        # Obtener la fecha actual en formato YYYY-MM-DD\n",
    "        fecha_actual = datetime.now().strftime('%Y-%m-%d')\n",
    "        # Crear el nombre del archivo\n",
    "        nombre_archivo = f'business_dunkin_{fecha_actual}.parquet'\n",
    "        # Guardar el DataFrame como archivo Parquet en el directorio especificado\n",
    "        df.to_parquet(f'{save_directory}/{nombre_archivo}')\n",
    "\n",
    "        # Comprobar si el archivo existe\n",
    "        archivo_antiguo = f'{save_directory}/business_dunkin.parquet'\n",
    "        if os.path.exists(archivo_antiguo):\n",
    "            # Cargar el archivo existente\n",
    "            df_antiguo = pd.read_parquet(archivo_antiguo)\n",
    "            \n",
    "            # Concatenar los datos que no están presentes en el archivo existente\n",
    "            df_nuevo = pd.concat([df_antiguo, df]).drop_duplicates().reset_index(drop=True)\n",
    "            \n",
    "            # Guardar el DataFrame actualizado con el mismo nombre\n",
    "            df_nuevo.to_parquet(archivo_antiguo)\n",
    "            \n",
    "            # Verificar si se actualizó la data\n",
    "            if len(df) > 0:\n",
    "                print(\"Se ha actualizado la data.\")\n",
    "            else:\n",
    "                print(\"No hay nuevos datos para actualizar.\")\n",
    "        else:\n",
    "            print(\"No se encontró el archivo antiguo. Guardando el nuevo archivo.\")\n",
    "\n",
    "    # Usar la función con el DataFrame procesado\n",
    "    guardar_y_comparar(df_business_dunkin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecucion de la Funcion\n",
    "procesar_y_guardar_dunkin('../data/business.pkl', '../gcp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL de la Data Review Starbucks Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def process_reviews_starbucks():\n",
    "    # Directorios y archivos\n",
    "    review_file_path = '../data/reviews.json'\n",
    "    business_file_path = '../gcp/business_starbucks.parquet'\n",
    "    output_dir = '../gcp/'\n",
    "    output_file_base = 'reviews_starbucks'\n",
    "    output_file_ext = '.parquet'\n",
    "    \n",
    "    # Leer el archivo reviews.json entero\n",
    "    with open(review_file_path, 'r') as f:\n",
    "        reviews = json.load(f)\n",
    "    df_reviews = pd.DataFrame(reviews)\n",
    "    \n",
    "    # Leer el archivo business_starbucks.parquet\n",
    "    df_business = pd.read_parquet(business_file_path)\n",
    "    \n",
    "    # Verificar que las columnas necesarias existan\n",
    "    if 'business_id' not in df_reviews.columns:\n",
    "        raise KeyError(\"La columna 'business_id' no se encuentra en el archivo reviews.json\")\n",
    "    if 'business_id' not in df_business.columns:\n",
    "        raise KeyError(\"La columna 'business_id' no se encuentra en el archivo business_starbucks.parquet\")\n",
    "    \n",
    "    # Cruzar ambos archivos por la columna business_id\n",
    "    df_reviews_starbucks = df_reviews[df_reviews['business_id'].isin(df_business['business_id'])]\n",
    "    \n",
    "    # Verificar que la columna 'date' exista\n",
    "    if 'date' not in df_reviews_starbucks.columns:\n",
    "        raise KeyError(\"La columna 'date' no se encuentra en el DataFrame resultante después del cruce\")\n",
    "    \n",
    "    # Eliminar la hora de la columna 'date', manteniendo el tipo datetime\n",
    "    df_reviews_starbucks['date'] = pd.to_datetime(df_reviews_starbucks['date']).dt.normalize()\n",
    "    \n",
    "    # Reiniciar el índice del DataFrame\n",
    "    df_reviews_starbucks.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Convertir a parquet sin guardar aún\n",
    "    current_date = datetime.now().strftime('%Y%m%d')\n",
    "    temp_output_file = os.path.join(output_dir, f\"{output_file_base}_{current_date}{output_file_ext}\")\n",
    "    df_reviews_starbucks.to_parquet(temp_output_file)\n",
    "    \n",
    "    final_output_file = os.path.join(output_dir, f\"{output_file_base}{output_file_ext}\")\n",
    "    \n",
    "    # Si el archivo reviews_starbucks.parquet ya existe\n",
    "    if os.path.exists(final_output_file):\n",
    "        existing_reviews = pd.read_parquet(final_output_file)\n",
    "        \n",
    "        # Combinar DataFrames y eliminar duplicados\n",
    "        combined_reviews = pd.concat([existing_reviews, df_reviews_starbucks]).drop_duplicates(subset=['review_id'])\n",
    "        \n",
    "        # Reiniciar índice\n",
    "        combined_reviews.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # Guardar el DataFrame combinado\n",
    "        combined_reviews.to_parquet(final_output_file)\n",
    "        print(\"La data se actualizó.\")\n",
    "    else:\n",
    "        # Renombrar el archivo temporal\n",
    "        os.rename(temp_output_file, final_output_file)\n",
    "        print(f\"El archivo se guardó como {final_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo se guardó como ../gcp/reviews_starbucks.parquet\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar la función\n",
    "process_reviews_starbucks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL de la Data Review Dunkin Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def process_reviews_dunkin():\n",
    "    # Directorios y archivos\n",
    "    review_file_path = '../data/reviews.json'\n",
    "    business_file_path = '../gcp/business_dunkin.parquet'\n",
    "    output_dir = '../gcp/'\n",
    "    output_file_base = 'reviews_dunkin'\n",
    "    output_file_ext = '.parquet'\n",
    "    \n",
    "    # Leer el archivo reviews.json entero\n",
    "    with open(review_file_path, 'r') as f:\n",
    "        reviews = json.load(f)\n",
    "    df_reviews = pd.DataFrame(reviews)\n",
    "    \n",
    "    # Leer el archivo business_dunkin.parquet\n",
    "    df_business = pd.read_parquet(business_file_path)\n",
    "    \n",
    "    # Verificar que las columnas necesarias existan\n",
    "    if 'business_id' not in df_reviews.columns:\n",
    "        raise KeyError(\"La columna 'business_id' no se encuentra en el archivo reviews.json\")\n",
    "    if 'business_id' not in df_business.columns:\n",
    "        raise KeyError(\"La columna 'business_id' no se encuentra en el archivo business_dunkin.parquet\")\n",
    "    \n",
    "    # Cruzar ambos archivos por la columna business_id\n",
    "    df_reviews_dunkin = df_reviews[df_reviews['business_id'].isin(df_business['business_id'])]\n",
    "    \n",
    "    # Verificar que la columna 'date' exista\n",
    "    if 'date' not in df_reviews_dunkin.columns:\n",
    "        raise KeyError(\"La columna 'date' no se encuentra en el DataFrame resultante después del cruce\")\n",
    "    \n",
    "    # Eliminar la hora de la columna 'date', manteniendo el tipo datetime\n",
    "    df_reviews_dunkin['date'] = pd.to_datetime(df_reviews_dunkin['date']).dt.normalize()\n",
    "    \n",
    "    # Reiniciar el índice del DataFrame\n",
    "    df_reviews_dunkin.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Convertir a parquet sin guardar aún\n",
    "    current_date = datetime.now().strftime('%Y%m%d')\n",
    "    temp_output_file = os.path.join(output_dir, f\"{output_file_base}_{current_date}{output_file_ext}\")\n",
    "    df_reviews_dunkin.to_parquet(temp_output_file)\n",
    "    \n",
    "    final_output_file = os.path.join(output_dir, f\"{output_file_base}{output_file_ext}\")\n",
    "    \n",
    "    # Si el archivo reviews_dunkin.parquet ya existe\n",
    "    if os.path.exists(final_output_file):\n",
    "        existing_reviews = pd.read_parquet(final_output_file)\n",
    "        \n",
    "        # Combinar DataFrames y eliminar duplicados\n",
    "        combined_reviews = pd.concat([existing_reviews, df_reviews_dunkin]).drop_duplicates(subset=['review_id'])\n",
    "        \n",
    "        # Reiniciar índice\n",
    "        combined_reviews.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # Guardar el DataFrame combinado\n",
    "        combined_reviews.to_parquet(final_output_file)\n",
    "        print(\"La data se actualizó.\")\n",
    "    else:\n",
    "        # Renombrar el archivo temporal\n",
    "        os.rename(temp_output_file, final_output_file)\n",
    "        print(f\"El archivo se guardó como {final_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo se guardó como ../gcp/reviews_dunkin.parquet\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar la función\n",
    "process_reviews_dunkin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL de la Data User Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def process_user_data():\n",
    "    # Definir tamaños de los lotes\n",
    "    batch_size = 10000  # Ajustar según la capacidad de memoria disponible\n",
    "\n",
    "    # Cargar los archivos de reviews\n",
    "    reviews_dunkin_df = pd.read_parquet('../gcp/reviews_dunkin.parquet')\n",
    "    reviews_starbucks_df = pd.read_parquet('../gcp/reviews_starbucks.parquet')\n",
    "\n",
    "    # Obtener los user_id únicos de los archivos de reviews\n",
    "    dunkin_user_ids = reviews_dunkin_df['user_id'].unique()\n",
    "    starbucks_user_ids = reviews_starbucks_df['user_id'].unique()\n",
    "\n",
    "    # Crear un set de user_ids únicos de ambos archivos de reviews\n",
    "    combined_user_ids = set(dunkin_user_ids).union(set(starbucks_user_ids))\n",
    "\n",
    "    # Inicializar el archivo Parquet\n",
    "    user_parquet_file = pq.ParquetFile('../data/user.parquet')\n",
    "\n",
    "    # Inicializar un writer para el archivo temporal\n",
    "    with pd.HDFStore('temp_user.h5', mode='w') as store:\n",
    "        max_lengths = {}\n",
    "\n",
    "        # Procesar el archivo user.parquet en lotes\n",
    "        for batch in user_parquet_file.iter_batches(batch_size):\n",
    "            # Convertir el batch en DataFrame\n",
    "            chunk = batch.to_pandas()\n",
    "\n",
    "            # Filtrar el chunk actual\n",
    "            filtered_chunk = chunk[chunk['user_id'].isin(combined_user_ids)]\n",
    "\n",
    "            # Obtener la longitud máxima de cada columna string en el chunk\n",
    "            for col in filtered_chunk.select_dtypes(include='object').columns:\n",
    "                max_len = filtered_chunk[col].str.len().max()\n",
    "                if col in max_lengths:\n",
    "                    max_lengths[col] = max(max_lengths[col], max_len)\n",
    "                else:\n",
    "                    max_lengths[col] = max_len\n",
    "\n",
    "        # Asegurarse de que cada longitud máxima sea al menos 20 (o un valor adecuado)\n",
    "        min_itemsize = {col: max(20, int(max_len) + 1) if pd.notna(max_len) else 20 for col, max_len in max_lengths.items()}\n",
    "\n",
    "        # Reinicializar la iteración sobre los lotes\n",
    "        for batch in user_parquet_file.iter_batches(batch_size):\n",
    "            # Convertir el batch en DataFrame\n",
    "            chunk = batch.to_pandas()\n",
    "\n",
    "            # Filtrar el chunk actual\n",
    "            filtered_chunk = chunk[chunk['user_id'].isin(combined_user_ids)]\n",
    "\n",
    "            # Escribir el chunk filtrado en el archivo temporal\n",
    "            store.append('filtered_user', filtered_chunk, data_columns=True, index=False, min_itemsize=min_itemsize)\n",
    "\n",
    "    # Cargar el archivo temporal y guardarlo como user.parquet en el directorio ..//gcp//\n",
    "    with pd.HDFStore('temp_user.h5', mode='r') as store:\n",
    "        filtered_user_df = store['filtered_user']\n",
    "\n",
    "    # Eliminar las columnas yelping_since, elite, friends, fans\n",
    "    filtered_user_df.drop(columns=['yelping_since', 'elite', 'friends', 'fans'], inplace=True)\n",
    "\n",
    "    # Resetear el índice\n",
    "    filtered_user_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Obtener la fecha actual\n",
    "    today_str = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Nombre del archivo de salida\n",
    "    output_file_path = f'../gcp/user_{today_str}.parquet'\n",
    "\n",
    "    # Guardar el resultado en el directorio ..//gcp// como user_fecha_actual.parquet\n",
    "    filtered_user_df.to_parquet(output_file_path)\n",
    "\n",
    "    # Verificar si existe el archivo user.parquet en el directorio ..//gcp//\n",
    "    existing_user_file_path = '../gcp/user.parquet'\n",
    "    if not os.path.exists(existing_user_file_path):\n",
    "        # Si no existe, renombrar el archivo actual\n",
    "        os.rename(output_file_path, existing_user_file_path)\n",
    "    else:\n",
    "        # Si existe, cruzar el archivo nuevo con user.parquet\n",
    "        existing_user_df = pd.read_parquet(existing_user_file_path)\n",
    "\n",
    "        # Filtrar los nuevos usuarios que no están en el archivo existente\n",
    "        new_users_df = filtered_user_df[~filtered_user_df['user_id'].isin(existing_user_df['user_id'])]\n",
    "\n",
    "        # Combinar los datos existentes con los nuevos usuarios\n",
    "        combined_user_df = pd.concat([existing_user_df, new_users_df], ignore_index=True)\n",
    "\n",
    "        # Guardar el archivo combinado\n",
    "        combined_user_df.to_parquet(existing_user_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamar a la función para procesar los datos\n",
    "process_user_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
