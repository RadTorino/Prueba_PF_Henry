{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar business.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el archivo .parquet en un DataFrame de Pandas\n",
    "df_business = pd.read_parquet('..//data//business.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparar el achivo review.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_split_json_to_parquet(json_file_path, output_prefix, num_parts):\n",
    "    # Contadores\n",
    "    line_count = 0\n",
    "    part_count = 0\n",
    "\n",
    "    # Lee el archivo JSON en chunks\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "        # Obtén el número total de líneas en el archivo\n",
    "        total_lines = sum(1 for line in file)\n",
    "        file.seek(0)\n",
    "\n",
    "        # Calcula el tamaño del chunk\n",
    "        chunk_size = total_lines // num_parts\n",
    "\n",
    "        # Inicializa una lista para almacenar los registros\n",
    "        records = []\n",
    "\n",
    "        for line in file:\n",
    "            # Parse la línea del JSON\n",
    "            record = json.loads(line.strip())\n",
    "            records.append(record)\n",
    "            line_count += 1\n",
    "\n",
    "            # Si hemos alcanzado el tamaño del chunk o es la última línea\n",
    "            if line_count % chunk_size == 0 or line_count == total_lines:\n",
    "                part_file_path = f\"{output_prefix}_part{part_count + 1}.parquet\"\n",
    "                \n",
    "                # Convertir la lista de registros a DataFrame\n",
    "                df_chunk = pd.DataFrame(records)\n",
    "                \n",
    "                # Convertir el DataFrame a tabla Arrow y escribir a Parquet\n",
    "                table = pa.Table.from_pandas(df_chunk)\n",
    "                pq.write_table(table, part_file_path)\n",
    "                print(f\"Part {part_count + 1} written to {part_file_path}\")\n",
    "                \n",
    "                # Resetear la lista de registros y aumentar el contador de partes\n",
    "                records = []\n",
    "                part_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 written to review_output_part1.parquet\n",
      "Part 2 written to review_output_part2.parquet\n",
      "Part 3 written to review_output_part3.parquet\n",
      "Part 4 written to review_output_part4.parquet\n",
      "Part 5 written to review_output_part5.parquet\n"
     ]
    }
   ],
   "source": [
    "# Usar la función\n",
    "convert_and_split_json_to_parquet('..//data//review.json', 'review_output', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de archivos .parquet\n",
    "file_paths = [\n",
    "    '..//code//review_output_part1.parquet',\n",
    "    '..//code//review_output_part2.parquet',\n",
    "    '..//code//review_output_part3.parquet',\n",
    "    '..//code//review_output_part4.parquet',\n",
    "    '..//code//review_output_part5.parquet'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered file written to ..//code//review_output_part1_filtered.parquet\n",
      "Filtered file written to ..//code//review_output_part2_filtered.parquet\n",
      "Filtered file written to ..//code//review_output_part3_filtered.parquet\n",
      "Filtered file written to ..//code//review_output_part4_filtered.parquet\n",
      "Filtered file written to ..//code//review_output_part5_filtered.parquet\n"
     ]
    }
   ],
   "source": [
    "# Filtrar cada DataFrame y almacenarlos en una lista\n",
    "filtered_dfs = []\n",
    "for file_path in file_paths:\n",
    "    # Leer el archivo .parquet\n",
    "    df_part = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Filtrar el DataFrame\n",
    "    df_part_filtered = df_part[df_part['business_id'].isin(df_business['business_id'])]\n",
    "    \n",
    "    # Agregar el DataFrame filtrado a la lista\n",
    "    filtered_dfs.append(df_part_filtered)\n",
    "    \n",
    "    # Guardar el DataFrame filtrado en un archivo .parquet\n",
    "    filtered_file_path = file_path.replace('.parquet', '_filtered.parquet')\n",
    "    df_part_filtered.to_parquet(filtered_file_path)\n",
    "    print(f\"Filtered file written to {filtered_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar los DataFrames filtrados\n",
    "df_review = pd.concat(filtered_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el DataFrame combinado en un archivo .parquet\n",
    "combined_file_path = '..//data//review.parquet'\n",
    "df_review.to_parquet(combined_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ya filtrado y guardado como .parquet y para los Starbuks de todo el pais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el archivo .parquet en un DataFrame de Pandas\n",
    "df_review = pd.read_parquet('..//data//review.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para normalizar y capitalizar texto\n",
    "def normalize_and_capitalize(text):\n",
    "    if text is None:\n",
    "        return ''\n",
    "    # Convertir a minúsculas y luego capitalizar la primera letra de cada palabra\n",
    "    normalized_text = ' '.join([word.capitalize() for word in text.lower().split()])\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de que la columna 'text' está convertida a str y luego aplicar la función\n",
    "df_review['text'] = df_review['text'].astype(str).apply(normalize_and_capitalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Ordered Caramel Frappe At Drive Thru, Big Mist...\n",
      "1    Drum-roll Please! Review #100 Coming Right Up!...\n",
      "2    We Stopped Here For My Chai And Hubby's Coffee...\n",
      "3    There's Been Three Times That I've Ordered A G...\n",
      "4    I Went In When They Had 4 People Working, Wait...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Verificar los primeros registros de la columna 'text' después de la transformación\n",
    "print(df_review['text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna 'date' a tipo datetime\n",
    "df_review['date'] = pd.to_datetime(df_review['date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2017-12-29 19:38:31\n",
      "1   2014-02-05 19:38:24\n",
      "2   2017-02-09 04:35:39\n",
      "3   2016-08-25 14:08:18\n",
      "4   2016-01-30 01:10:42\n",
      "Name: date, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Verificar los primeros registros de la columna 'date' después de la conversión\n",
    "print(df_review['date'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restablecer el índice\n",
    "df_review = df_review.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21739 entries, 0 to 21738\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   review_id    21739 non-null  object        \n",
      " 1   user_id      21739 non-null  object        \n",
      " 2   business_id  21739 non-null  object        \n",
      " 3   stars        21739 non-null  float64       \n",
      " 4   useful       21739 non-null  int64         \n",
      " 5   funny        21739 non-null  int64         \n",
      " 6   cool         21739 non-null  int64         \n",
      " 7   text         21739 non-null  object        \n",
      " 8   date         21739 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(1), int64(3), object(4)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_review.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ULzbgT5SPThiMOoo4a_Adw</td>\n",
       "      <td>veBX5roHQLNnQ1rTtNh_gg</td>\n",
       "      <td>aJvxWyQIG5OLfBw3qAe8xA</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ordered Caramel Frappe At Drive Thru, Big Mist...</td>\n",
       "      <td>2017-12-29 19:38:31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  ULzbgT5SPThiMOoo4a_Adw  veBX5roHQLNnQ1rTtNh_gg  aJvxWyQIG5OLfBw3qAe8xA   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0    2.0       0      0     0   \n",
       "\n",
       "                                                text                date  \n",
       "0  Ordered Caramel Frappe At Drive Thru, Big Mist... 2017-12-29 19:38:31  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el DataFrame como archivo Parquet\n",
    "df_review.to_parquet('..//data//review.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
