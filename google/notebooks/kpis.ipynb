{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common pattern is: slow service\n",
      "Frequency: 121\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'merged_starbucks_reviews_with_state.parquet'\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# Filter negative reviews\n",
    "negative_reviews = df[df['avg_rating'] < 3]['text'].dropna()\n",
    "\n",
    "# Define patterns to search for\n",
    "patterns = {\n",
    "    'slow service': ['slow', 'long', 'line', 'slowest'],\n",
    "    'rude staff': ['rude', 'bad', 'unfriendly', 'horrible', 'customer', 'nasty'],\n",
    "    'incorrect orders': ['wrong', 'size', 'messed', 'incorrect'],\n",
    "    'lack of product availability': ['stock', 'no cups', 'no lids', 'no ingredients', 'ran out'],\n",
    "    'high prices': ['expensive', 'price', 'overpriced', 'expensive']\n",
    "}\n",
    "\n",
    "# Count the frequency of each pattern\n",
    "pattern_counts = Counter()\n",
    "for review in negative_reviews:\n",
    "    for pattern, keywords in patterns.items():\n",
    "        if any(re.search(keyword, review, re.IGNORECASE) for keyword in keywords):\n",
    "            pattern_counts[pattern] += 1\n",
    "\n",
    "# Check if there are elements in the counter before trying to access them\n",
    "if pattern_counts:\n",
    "    most_common_pattern = pattern_counts.most_common(1)[0]\n",
    "    print('The most common pattern is:', most_common_pattern[0])\n",
    "    print('Frequency:', most_common_pattern[1])\n",
    "else:\n",
    "    print('No common patterns found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La palabra \"slow\" se repite: 2283.0 veces en la columna \"text\".\n"
     ]
    }
   ],
   "source": [
    "# Contar la frecuencia de la palabra 'slow' en la columna 'text'\n",
    "slow_count = df['text'].str.count('slow', flags=re.IGNORECASE).sum()\n",
    "print('La palabra \"slow\" se repite:', slow_count, 'veces en la columna \"text\".')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **MODELOS DE PREDICCION RNNs y CNNs.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Medimos que la puntuación promedio de Starbucks sea superior en un 10% a la de su principal competidor(dunkin), medida por semestre(entre el ultimo semestre y el semestre futuro)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zimme\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 590us/step - loss: 2.6842\n",
      "Epoch 2/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 590us/step - loss: 2.7250e-06\n",
      "Epoch 3/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 588us/step - loss: 6.9828e-09\n",
      "Epoch 4/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 585us/step - loss: 4.8784e-14\n",
      "Epoch 5/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 599us/step - loss: 0.0000e+00\n",
      "Epoch 6/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 596us/step - loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 601us/step - loss: 0.0000e+00\n",
      "Epoch 8/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 598us/step - loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 595us/step - loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 594us/step - loss: 0.0000e+00\n",
      "\u001b[1m980/980\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 422us/step - loss: 0.0000e+00\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "Pérdida en el conjunto de prueba (RNN): 0.0\n",
      "Predicción para el próximo semestre (RNN): 4.0\n",
      "Diferencia porcentual entre la predicción del modelo RNN y la puntuación de Dunkin en el último semestre: -0.8991521546729806\n",
      "Diferencia absoluta en puntuaciones (RNN): 0.03629241017499041\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargar los datos\n",
    "starbucks_df = pd.read_parquet('../data/Starbucks_reviews_ETL_limpio.parquet')\n",
    "dunkin_df = pd.read_parquet('../data/Dunkin_reviews_ETL_limpio.parquet')\n",
    "\n",
    "# Preparar los datos\n",
    "X_starbucks = starbucks_df['rating'].values.reshape(-1, 1)\n",
    "y_starbucks = starbucks_df['rating'].values\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_starbucks, y_starbucks, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir el modelo RNN\n",
    "model_rnn = Sequential([\n",
    "    SimpleRNN(50, activation='relu', input_shape=(1, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model_rnn.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Entrenar el modelo\n",
    "model_rnn.fit(X_train, y_train, epochs=10, verbose=1)\n",
    "\n",
    "# Evaluar el modelo\n",
    "loss_rnn = model_rnn.evaluate(X_test, y_test)\n",
    "\n",
    "# Hacer predicciones para el semestre futuro\n",
    "next_semester_prediction_rnn = model_rnn.predict(np.array([[4.0]]))  # Usamos 4.0 como ejemplo\n",
    "\n",
    "# Comparar con la puntuación promedio de Dunkin en el último semestre\n",
    "dunkin_avg_last_semester = dunkin_df['rating'].mean()\n",
    "\n",
    "# Calcular la diferencia porcentual y absoluta\n",
    "percentage_difference_rnn = ((next_semester_prediction_rnn[0][0] - dunkin_avg_last_semester) / dunkin_avg_last_semester) * 100\n",
    "absolute_difference_rnn = dunkin_avg_last_semester - next_semester_prediction_rnn[0][0]\n",
    "\n",
    "print('Pérdida en el conjunto de prueba (RNN):', loss_rnn)\n",
    "print('Predicción para el próximo semestre (RNN):', next_semester_prediction_rnn[0][0])\n",
    "print('Diferencia porcentual entre la predicción del modelo RNN y la puntuación de Dunkin en el último semestre:', percentage_difference_rnn)\n",
    "print('Diferencia absoluta en puntuaciones (RNN):', absolute_difference_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 587us/step - loss: 2.3290\n",
      "Epoch 2/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 591us/step - loss: 2.4437e-06\n",
      "Epoch 3/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 591us/step - loss: 1.9845e-07\n",
      "Epoch 4/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 589us/step - loss: 4.1966e-07\n",
      "Epoch 5/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 583us/step - loss: 2.0151e-06\n",
      "Epoch 6/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 590us/step - loss: 1.3163e-06\n",
      "Epoch 7/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 594us/step - loss: 1.0273e-06\n",
      "Epoch 8/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 603us/step - loss: 2.0304e-06\n",
      "Epoch 9/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 593us/step - loss: 1.0907e-06\n",
      "Epoch 10/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 589us/step - loss: 2.5278e-06\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "Epoch 1/10\n",
      "\u001b[1m2898/2898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 592us/step - loss: 1.9767\n",
      "Epoch 2/10\n",
      "\u001b[1m2898/2898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 587us/step - loss: 7.9654e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m2898/2898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 591us/step - loss: 9.5628e-14\n",
      "Epoch 4/10\n",
      "\u001b[1m2898/2898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 590us/step - loss: 1.8764e-14\n",
      "Epoch 5/10\n",
      "\u001b[1m2898/2898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 587us/step - loss: 3.4904e-14\n",
      "Epoch 6/10\n",
      "\u001b[1m2898/2898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 585us/step - loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "\u001b[1m2898/2898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 594us/step - loss: 0.0000e+00\n",
      "Epoch 8/10\n",
      "\u001b[1m2898/2898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 583us/step - loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "\u001b[1m2898/2898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 594us/step - loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "\u001b[1m2898/2898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 573us/step - loss: 0.0000e+00\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "Predicción para el próximo semestre (RNN) de Starbucks: 4.0003347\n",
      "Predicción para el próximo semestre (RNN) de Dunkin: 4.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Preparar los datos para Starbucks\n",
    "X_starbucks = starbucks_df['rating'].values.reshape(-1, 1)\n",
    "y_starbucks = starbucks_df['rating'].values\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train_starbucks, X_test_starbucks, y_train_starbucks, y_test_starbucks = train_test_split(X_starbucks, y_starbucks, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir el modelo RNN para Starbucks\n",
    "model_rnn_starbucks = Sequential([\n",
    "    SimpleRNN(50, activation='relu', input_shape=(1, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model_rnn_starbucks.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Entrenar el modelo\n",
    "model_rnn_starbucks.fit(X_train_starbucks, y_train_starbucks, epochs=10, verbose=1)\n",
    "\n",
    "# Hacer predicciones para el semestre futuro de Starbucks\n",
    "next_semester_prediction_rnn_starbucks = model_rnn_starbucks.predict(np.array([[4.0]]))  # Usamos 4.0 como ejemplo\n",
    "\n",
    "# Preparar los datos para Dunkin\n",
    "X_dunkin = dunkin_df['rating'].values.reshape(-1, 1)\n",
    "y_dunkin = dunkin_df['rating'].values\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train_dunkin, X_test_dunkin, y_train_dunkin, y_test_dunkin = train_test_split(X_dunkin, y_dunkin, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir el modelo RNN para Dunkin\n",
    "model_rnn_dunkin = Sequential([\n",
    "    SimpleRNN(50, activation='relu', input_shape=(1, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model_rnn_dunkin.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Entrenar el modelo\n",
    "model_rnn_dunkin.fit(X_train_dunkin, y_train_dunkin, epochs=10, verbose=1)\n",
    "\n",
    "# Hacer predicciones para el semestre futuro de Dunkin\n",
    "next_semester_prediction_rnn_dunkin = model_rnn_dunkin.predict(np.array([[4.0]]))  # Usamos 4.0 como ejemplo\n",
    "\n",
    "print('Predicción para el próximo semestre (RNN) de Starbucks:', next_semester_prediction_rnn_starbucks[0][0])\n",
    "print('Predicción para el próximo semestre (RNN) de Dunkin:', next_semester_prediction_rnn_dunkin[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-CNNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zimme\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 685us/step - loss: 0.3502\n",
      "Epoch 2/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 680us/step - loss: 7.7069e-13\n",
      "Epoch 3/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 679us/step - loss: 3.5066e-06\n",
      "Epoch 4/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 677us/step - loss: 1.6046e-05\n",
      "Epoch 5/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 681us/step - loss: 1.3463e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 689us/step - loss: 8.0874e-06\n",
      "Epoch 7/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 681us/step - loss: 9.9040e-06\n",
      "Epoch 8/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 687us/step - loss: 1.2647e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 685us/step - loss: 1.4092e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 689us/step - loss: 8.7441e-06\n",
      "\u001b[1m980/980\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 428us/step - loss: 6.6780e-05\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Pérdida en el conjunto de prueba (CNN): 6.693222530884668e-05\n",
      "Predicción para el próximo semestre (CNN): 3.992558\n",
      "Diferencia porcentual entre la predicción del modelo CNN y la puntuación de Dunkin en el último semestre: -1.0835292208467748\n",
      "Diferencia absoluta en puntuaciones (CNN): 0.04373440770306658\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Preparar los datos\n",
    "X_starbucks = starbucks_df['rating'].values.reshape(-1, 1)\n",
    "y_starbucks = starbucks_df['rating'].values\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_starbucks, y_starbucks, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir el modelo CNN\n",
    "model_cnn = Sequential([\n",
    "    Conv1D(64, kernel_size=1, activation='relu', input_shape=(1, 1)),  # Cambio de kernel_size a 1\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model_cnn.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Entrenar el modelo\n",
    "model_cnn.fit(X_train, y_train, epochs=10, verbose=1)\n",
    "\n",
    "# Evaluar el modelo\n",
    "loss_cnn = model_cnn.evaluate(X_test, y_test)\n",
    "\n",
    "# Hacer predicciones para el semestre futuro\n",
    "next_semester_prediction_cnn = model_cnn.predict(np.array([[4.0]]))  # Usamos 4.0 como ejemplo\n",
    "\n",
    "# Comparar con la puntuación promedio de Dunkin en el último semestre\n",
    "dunkin_avg_last_semester = dunkin_df['rating'].mean()\n",
    "\n",
    "# Calcular la diferencia porcentual y absoluta\n",
    "percentage_difference_cnn = ((next_semester_prediction_cnn[0][0] - dunkin_avg_last_semester) / dunkin_avg_last_semester) * 100\n",
    "absolute_difference_cnn = dunkin_avg_last_semester - next_semester_prediction_cnn[0][0]\n",
    "\n",
    "print('Pérdida en el conjunto de prueba (CNN):', loss_cnn)\n",
    "print('Predicción para el próximo semestre (CNN):', next_semester_prediction_cnn[0][0])\n",
    "print('Diferencia porcentual entre la predicción del modelo CNN y la puntuación de Dunkin en el último semestre:', percentage_difference_cnn)\n",
    "print('Diferencia absoluta en puntuaciones (CNN):', absolute_difference_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 678us/step - loss: 0.4866\n",
      "Epoch 2/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 681us/step - loss: 3.7530e-08\n",
      "Epoch 3/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 699us/step - loss: 4.7638e-06\n",
      "Epoch 4/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 689us/step - loss: 1.1228e-05\n",
      "Epoch 5/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 682us/step - loss: 1.1547e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 671us/step - loss: 1.4433e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 678us/step - loss: 1.1606e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 668us/step - loss: 1.4721e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 674us/step - loss: 1.3086e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m3919/3919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 675us/step - loss: 1.3066e-05\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001C6C35DA8E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Epoch 1/10\n",
      "\u001b[1m2898/2898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 675us/step - loss: 0.4260\n",
      "Epoch 2/10\n",
      "\u001b[1m2898/2898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 684us/step - loss: 1.0208e-14\n",
      "Epoch 3/10\n",
      "\u001b[1m2898/2898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 695us/step - loss: 8.7331e-17\n",
      "Epoch 4/10\n",
      "\u001b[1m2898/2898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 683us/step - loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "\u001b[1m2898/2898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 690us/step - loss: 0.0000e+00\n",
      "Epoch 6/10\n",
      "\u001b[1m2898/2898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 680us/step - loss: 4.2298e-22\n",
      "Epoch 7/10\n",
      "\u001b[1m2898/2898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 690us/step - loss: 8.9406e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m2898/2898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 694us/step - loss: 6.1644e-06\n",
      "Epoch 9/10\n",
      "\u001b[1m2898/2898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 675us/step - loss: 1.9471e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m2898/2898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 679us/step - loss: 7.8229e-06\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001C6C4875440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicción para el próximo semestre (CNN) de Starbucks: 3.9998274\n",
      "Predicción para el próximo semestre (CNN) de Dunkin: 4.000011\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv1D, Flatten\n",
    "\n",
    "# Definir el modelo CNN para Starbucks\n",
    "model_cnn_starbucks = Sequential([\n",
    "    Conv1D(64, kernel_size=1, activation='relu', input_shape=(1, 1)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model_cnn_starbucks.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Entrenar el modelo\n",
    "model_cnn_starbucks.fit(X_train_starbucks, y_train_starbucks, epochs=10, verbose=1)\n",
    "\n",
    "# Hacer predicciones para el semestre futuro de Starbucks\n",
    "next_semester_prediction_cnn_starbucks = model_cnn_starbucks.predict(np.array([[4.0]]))  # Usamos 4.0 como ejemplo\n",
    "\n",
    "# Definir el modelo CNN para Dunkin\n",
    "model_cnn_dunkin = Sequential([\n",
    "    Conv1D(64, kernel_size=1, activation='relu', input_shape=(1, 1)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model_cnn_dunkin.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Entrenar el modelo\n",
    "model_cnn_dunkin.fit(X_train_dunkin, y_train_dunkin, epochs=10, verbose=1)\n",
    "\n",
    "# Hacer predicciones para el semestre futuro de Dunkin\n",
    "next_semester_prediction_cnn_dunkin = model_cnn_dunkin.predict(np.array([[4.0]]))  # Usamos 4.0 como ejemplo\n",
    "\n",
    "print('Predicción para el próximo semestre (CNN) de Starbucks:', next_semester_prediction_cnn_starbucks[0][0])\n",
    "print('Predicción para el próximo semestre (CNN) de Dunkin:', next_semester_prediction_cnn_dunkin[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Promedios de cada compañia en el ultimo semestre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de Starbucks en el último semestre: 4.28763353836193\n",
      "Promedio de Dunkin en el último semestre: 4.065958576739246\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Filtrar los datos del último semestre\n",
    "# Asumiendo que la columna 'date' contiene las fechas y está en formato datetime\n",
    "starbucks_df['date'] = pd.to_datetime(starbucks_df['date'])\n",
    "dunkin_df['date'] = pd.to_datetime(dunkin_df['date'])\n",
    "\n",
    "# Obtener la fecha más reciente en los datos\n",
    "max_date_starbucks = starbucks_df['date'].max()\n",
    "max_date_dunkin = dunkin_df['date'].max()\n",
    "\n",
    "# Calcular la fecha de inicio del último semestre\n",
    "start_date_starbucks = max_date_starbucks - pd.DateOffset(months=6)\n",
    "start_date_dunkin = max_date_dunkin - pd.DateOffset(months=6)\n",
    "\n",
    "# Filtrar los datos del último semestre\n",
    "last_semester_starbucks = starbucks_df[starbucks_df['date'] >= start_date_starbucks]\n",
    "last_semester_dunkin = dunkin_df[dunkin_df['date'] >= start_date_dunkin]\n",
    "\n",
    "# Calcular los promedios\n",
    "average_starbucks = last_semester_starbucks['rating'].mean()\n",
    "average_dunkin = last_semester_dunkin['rating'].mean()\n",
    "\n",
    "print('Promedio de Starbucks en el último semestre:', average_starbucks)\n",
    "print('Promedio de Dunkin en el último semestre:', average_dunkin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Diferencias porcentuales de las predicciones de los ratings de ambas compañías para los dos modelos (RNNs y CNNs) con los promedios de cada compañía en el último semestre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diferencia porcentual (RNN) de Starbucks: -6.708445014911857\n",
      "Diferencia porcentual (RNN) de Dunkin: -1.6222146756876776\n",
      "Diferencia porcentual (CNN) de Starbucks: -6.708445014911857\n",
      "Diferencia porcentual (CNN) de Dunkin: -1.6064496355894593\n"
     ]
    }
   ],
   "source": [
    "# Predicciones de los modelos\n",
    "pred_rnn_starbucks = 4.0\n",
    "pred_rnn_dunkin = 4.0\n",
    "pred_cnn_starbucks = 4.0\n",
    "pred_cnn_dunkin = 4.000641\n",
    "\n",
    "# Promedios del último semestre\n",
    "avg_starbucks = 4.28763353836193\n",
    "avg_dunkin = 4.065958576739246\n",
    "\n",
    "# Calcular las diferencias porcentuales\n",
    "perc_diff_rnn_starbucks = ((pred_rnn_starbucks - avg_starbucks) / avg_starbucks) * 100\n",
    "perc_diff_rnn_dunkin = ((pred_rnn_dunkin - avg_dunkin) / avg_dunkin) * 100\n",
    "perc_diff_cnn_starbucks = ((pred_cnn_starbucks - avg_starbucks) / avg_starbucks) * 100\n",
    "perc_diff_cnn_dunkin = ((pred_cnn_dunkin - avg_dunkin) / avg_dunkin) * 100\n",
    "\n",
    "print('Diferencia porcentual (RNN) de Starbucks:', perc_diff_rnn_starbucks)\n",
    "print('Diferencia porcentual (RNN) de Dunkin:', perc_diff_rnn_dunkin)\n",
    "print('Diferencia porcentual (CNN) de Starbucks:', perc_diff_cnn_starbucks)\n",
    "print('Diferencia porcentual (CNN) de Dunkin:', perc_diff_cnn_dunkin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cumplimiento del KPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿El promedio de Starbucks es superior en un 10% al de Dunkin para el próximo semestre (RNN)? False\n",
      "¿El promedio de Starbucks es superior en un 10% al de Dunkin para el próximo semestre (CNN)? False\n"
     ]
    }
   ],
   "source": [
    "# Calcular el 10% del promedio de Dunkin para el próximo semestre (predicción RNN)\n",
    "threshold_rnn_next_semester = pred_rnn_dunkin * 1.10\n",
    "\n",
    "# Comparar con la predicción de Starbucks para el próximo semestre (predicción RNN)\n",
    "is_above_threshold_rnn_next_semester = pred_rnn_starbucks > threshold_rnn_next_semester\n",
    "\n",
    "# Calcular el 10% del promedio de Dunkin para el próximo semestre (predicción CNN)\n",
    "threshold_cnn_next_semester = pred_cnn_dunkin * 1.10\n",
    "\n",
    "# Comparar con la predicción de Starbucks para el próximo semestre (predicción CNN)\n",
    "is_above_threshold_cnn_next_semester = pred_cnn_starbucks > threshold_cnn_next_semester\n",
    "\n",
    "print('¿El promedio de Starbucks es superior en un 10% al de Dunkin para el próximo semestre (RNN)?', is_above_threshold_rnn_next_semester)\n",
    "print('¿El promedio de Starbucks es superior en un 10% al de Dunkin para el próximo semestre (CNN)?', is_above_threshold_cnn_next_semester)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
